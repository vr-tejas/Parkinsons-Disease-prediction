{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3mU_N4AVxtp",
        "outputId": "5a674d9e-fc82-4cfb-c918-e478f99fe5bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive already mounted.\n",
            "PD images: 290\n",
            "Control images: 26\n",
            "Parkinson's images: 215\n",
            "Control images: 26\n",
            "ðŸ“ Saving all results to: /content/drive/MyDrive/parkinsons_lightweight_model_results\n",
            "\n",
            "=== PREPARING DATASET ===\n",
            "Original dataset - PD: 215, Control: 26\n",
            "Split dataset:\n",
            "Train set: 150 PD, 18 control\n",
            "Validation set: 32 PD, 4 control\n",
            "Test set: 33 PD, 4 control\n",
            "Class weights - Control: 2.33, PD: 0.84\n",
            "\n",
            "=== CREATING ADVANCED ENSEMBLE MODEL ===\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Model created with 6,924,953 parameters\n",
            "\n",
            "Training advanced ensemble model...\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 4.999999873689376e-05.\n",
            "Epoch 1/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4421 - auc: 0.4907 - loss: 2.3791 - precision: 0.8761 - recall: 0.4270\n",
            "Epoch 1: val_auc improved from -inf to 0.77344, saving model to /content/drive/MyDrive/parkinsons_lightweight_model_results/best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 2s/step - accuracy: 0.4432 - auc: 0.4873 - loss: 2.3795 - precision: 0.8762 - recall: 0.4291 - val_accuracy: 0.8889 - val_auc: 0.7734 - val_loss: 1.7937 - val_precision: 0.8889 - val_recall: 1.0000 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 5.9999998989515005e-05.\n",
            "Epoch 2/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5020 - auc: 0.5932 - loss: 2.2273 - precision: 0.8980 - recall: 0.4976\n",
            "Epoch 2: val_auc improved from 0.77344 to 0.91797, saving model to /content/drive/MyDrive/parkinsons_lightweight_model_results/best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.5004 - auc: 0.5888 - loss: 2.2288 - precision: 0.8975 - recall: 0.4959 - val_accuracy: 0.8889 - val_auc: 0.9180 - val_loss: 1.8497 - val_precision: 0.8889 - val_recall: 1.0000 - learning_rate: 6.0000e-05\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 7.59999990905635e-05.\n",
            "Epoch 3/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5054 - auc: 0.5480 - loss: 2.2157 - precision: 0.8856 - recall: 0.4997\n",
            "Epoch 3: val_auc did not improve from 0.91797\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.5059 - auc: 0.5518 - loss: 2.2135 - precision: 0.8875 - recall: 0.5000 - val_accuracy: 0.8889 - val_auc: 0.9180 - val_loss: 1.9185 - val_precision: 0.8889 - val_recall: 1.0000 - learning_rate: 7.6000e-05\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 9.03999986499548e-05.\n",
            "Epoch 4/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942ms/step - accuracy: 0.4955 - auc: 0.5364 - loss: 2.2588 - precision: 0.8708 - recall: 0.5107\n",
            "Epoch 4: val_auc did not improve from 0.91797\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.4968 - auc: 0.5421 - loss: 2.2535 - precision: 0.8738 - recall: 0.5098 - val_accuracy: 0.9167 - val_auc: 0.8555 - val_loss: 1.9637 - val_precision: 0.9394 - val_recall: 0.9688 - learning_rate: 9.0400e-05\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 9.807999942684546e-05.\n",
            "Epoch 5/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5673 - auc: 0.6758 - loss: 2.1863 - precision: 0.9604 - recall: 0.5385\n",
            "Epoch 5: val_auc did not improve from 0.91797\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.5659 - auc: 0.6699 - loss: 2.1904 - precision: 0.9572 - recall: 0.5382 - val_accuracy: 0.9444 - val_auc: 0.7266 - val_loss: 2.0336 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 9.8080e-05\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969ms/step - accuracy: 0.5933 - auc: 0.7114 - loss: 2.0918 - precision: 0.9780 - recall: 0.5790\n",
            "Epoch 6: val_auc did not improve from 0.91797\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.5903 - auc: 0.7080 - loss: 2.0958 - precision: 0.9762 - recall: 0.5762 - val_accuracy: 0.9444 - val_auc: 0.7305 - val_loss: 2.0402 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 9.999999949475751e-05.\n",
            "Epoch 7/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957ms/step - accuracy: 0.6059 - auc: 0.6980 - loss: 2.1595 - precision: 0.9483 - recall: 0.5846\n",
            "Epoch 7: val_auc did not improve from 0.91797\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.6008 - auc: 0.6940 - loss: 2.1629 - precision: 0.9459 - recall: 0.5795 - val_accuracy: 0.9444 - val_auc: 0.7461 - val_loss: 2.0123 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 9.999999898951501e-05.\n",
            "Epoch 8/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949ms/step - accuracy: 0.5576 - auc: 0.5992 - loss: 2.1999 - precision: 0.9114 - recall: 0.5575\n",
            "Epoch 8: val_auc did not improve from 0.91797\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.5595 - auc: 0.6047 - loss: 2.1966 - precision: 0.9136 - recall: 0.5587 - val_accuracy: 0.9444 - val_auc: 0.8555 - val_loss: 1.9924 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 8.199999675620347e-05.\n",
            "Epoch 9/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936ms/step - accuracy: 0.5775 - auc: 0.7403 - loss: 2.0956 - precision: 0.9303 - recall: 0.5558\n",
            "Epoch 9: val_auc did not improve from 0.91797\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.5786 - auc: 0.7406 - loss: 2.0949 - precision: 0.9317 - recall: 0.5570 - val_accuracy: 0.9444 - val_auc: 0.8945 - val_loss: 1.9705 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 8.2000e-05\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 8.559999892488123e-05.\n",
            "Epoch 10/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983ms/step - accuracy: 0.5590 - auc: 0.6288 - loss: 2.1771 - precision: 0.9339 - recall: 0.5299\n",
            "Epoch 10: val_auc did not improve from 0.91797\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.5598 - auc: 0.6311 - loss: 2.1738 - precision: 0.9349 - recall: 0.5316 - val_accuracy: 0.9444 - val_auc: 0.7891 - val_loss: 1.9959 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 8.5600e-05\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 8.55999969644472e-05.\n",
            "Epoch 11/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950ms/step - accuracy: 0.5713 - auc: 0.7144 - loss: 2.0934 - precision: 0.9371 - recall: 0.5557\n",
            "Epoch 11: val_auc did not improve from 0.91797\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.5705 - auc: 0.7120 - loss: 2.0945 - precision: 0.9377 - recall: 0.5543 - val_accuracy: 0.9444 - val_auc: 0.7383 - val_loss: 2.0089 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 8.5600e-05\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 8.847999757155777e-05.\n",
            "Epoch 12/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969ms/step - accuracy: 0.6299 - auc: 0.8317 - loss: 2.0452 - precision: 0.9939 - recall: 0.5926\n",
            "Epoch 12: val_auc did not improve from 0.91797\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.6276 - auc: 0.8283 - loss: 2.0474 - precision: 0.9935 - recall: 0.5908 - val_accuracy: 0.9444 - val_auc: 0.7422 - val_loss: 1.9507 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 8.8480e-05\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 9.308799736667425e-05.\n",
            "Epoch 13/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6157 - auc: 0.7110 - loss: 2.0762 - precision: 0.9353 - recall: 0.5989   \n",
            "Epoch 13: val_auc did not improve from 0.91797\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.6151 - auc: 0.7131 - loss: 2.0760 - precision: 0.9364 - recall: 0.5979 - val_accuracy: 0.9444 - val_auc: 0.8711 - val_loss: 1.9038 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 9.3088e-05\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 9.723519912455231e-05.\n",
            "Epoch 14/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987ms/step - accuracy: 0.5562 - auc: 0.7135 - loss: 2.0720 - precision: 0.9351 - recall: 0.5454\n",
            "Epoch 14: val_auc improved from 0.91797 to 0.98828, saving model to /content/drive/MyDrive/parkinsons_lightweight_model_results/best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.5610 - auc: 0.7198 - loss: 2.0691 - precision: 0.9372 - recall: 0.5500 - val_accuracy: 0.9444 - val_auc: 0.9883 - val_loss: 1.8749 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 9.7235e-05\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 9.944703981280327e-05.\n",
            "Epoch 15/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6440 - auc: 0.8506 - loss: 1.9993 - precision: 0.9723 - recall: 0.6159\n",
            "Epoch 15: val_auc did not improve from 0.98828\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 6.961292747291735e-05.\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.6452 - auc: 0.8486 - loss: 2.0005 - precision: 0.9729 - recall: 0.6167 - val_accuracy: 0.9444 - val_auc: 0.9766 - val_loss: 1.8713 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 9.9447e-05\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 16/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6510 - auc: 0.8693 - loss: 1.9749 - precision: 0.9872 - recall: 0.6109\n",
            "Epoch 16: val_auc did not improve from 0.98828\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.6502 - auc: 0.8672 - loss: 1.9779 - precision: 0.9865 - recall: 0.6108 - val_accuracy: 0.9444 - val_auc: 0.8516 - val_loss: 1.8675 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 9.999999949475751e-05.\n",
            "Epoch 17/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6694 - auc: 0.7721 - loss: 2.0875 - precision: 0.9464 - recall: 0.6667\n",
            "Epoch 17: val_auc did not improve from 0.98828\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.6686 - auc: 0.7742 - loss: 2.0847 - precision: 0.9470 - recall: 0.6653 - val_accuracy: 0.9444 - val_auc: 0.9453 - val_loss: 1.9041 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 9.999999898951501e-05.\n",
            "Epoch 18/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998ms/step - accuracy: 0.6296 - auc: 0.8579 - loss: 1.9760 - precision: 0.9917 - recall: 0.5982\n",
            "Epoch 18: val_auc did not improve from 0.98828\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.6321 - auc: 0.8550 - loss: 1.9780 - precision: 0.9908 - recall: 0.6011 - val_accuracy: 0.9444 - val_auc: 0.8828 - val_loss: 1.9390 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 9.99999984842725e-05.\n",
            "Epoch 19/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6069 - auc: 0.7298 - loss: 2.1162 - precision: 0.9640 - recall: 0.5798\n",
            "Epoch 19: val_auc did not improve from 0.98828\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.6074 - auc: 0.7364 - loss: 2.1109 - precision: 0.9652 - recall: 0.5796 - val_accuracy: 0.9444 - val_auc: 0.9727 - val_loss: 1.9395 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 9.999999797903001e-05.\n",
            "Epoch 20/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993ms/step - accuracy: 0.6789 - auc: 0.9565 - loss: 1.9287 - precision: 1.0000 - recall: 0.6447\n",
            "Epoch 20: val_auc did not improve from 0.98828\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.6777 - auc: 0.9571 - loss: 1.9270 - precision: 1.0000 - recall: 0.6434 - val_accuracy: 0.9722 - val_auc: 0.9688 - val_loss: 1.9529 - val_precision: 1.0000 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 21/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6589 - auc: 0.8329 - loss: 2.0140 - precision: 0.9533 - recall: 0.6352   \n",
            "Epoch 21: val_auc did not improve from 0.98828\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.6579 - auc: 0.8342 - loss: 2.0133 - precision: 0.9547 - recall: 0.6341 - val_accuracy: 0.9444 - val_auc: 0.9688 - val_loss: 1.9336 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 9.999999797903001e-05.\n",
            "Epoch 22/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7022 - auc: 0.8188 - loss: 1.9890 - precision: 0.9709 - recall: 0.6883   \n",
            "Epoch 22: val_auc did not improve from 0.98828\n",
            "\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.7019 - auc: 0.8240 - loss: 1.9850 - precision: 0.9711 - recall: 0.6879 - val_accuracy: 0.9444 - val_auc: 0.9688 - val_loss: 1.8872 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 8.199999675620348e-05.\n",
            "Epoch 23/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995ms/step - accuracy: 0.7019 - auc: 0.9358 - loss: 1.9170 - precision: 1.0000 - recall: 0.6555\n",
            "Epoch 23: val_auc did not improve from 0.98828\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.6984 - auc: 0.9353 - loss: 1.9193 - precision: 1.0000 - recall: 0.6524 - val_accuracy: 0.9444 - val_auc: 0.9688 - val_loss: 1.8589 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 8.2000e-05\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 9.27999994624406e-05.\n",
            "Epoch 24/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982ms/step - accuracy: 0.7305 - auc: 0.9382 - loss: 1.9157 - precision: 1.0000 - recall: 0.6988\n",
            "Epoch 24: val_auc did not improve from 0.98828\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.7279 - auc: 0.9369 - loss: 1.9175 - precision: 1.0000 - recall: 0.6960 - val_accuracy: 0.9444 - val_auc: 0.9688 - val_loss: 1.8169 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 9.2800e-05\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 9.856000017141924e-05.\n",
            "Epoch 25/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993ms/step - accuracy: 0.6564 - auc: 0.7759 - loss: 2.0020 - precision: 0.9828 - recall: 0.6345\n",
            "Epoch 25: val_auc did not improve from 0.98828\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.6580 - auc: 0.7810 - loss: 1.9999 - precision: 0.9811 - recall: 0.6367 - val_accuracy: 0.9444 - val_auc: 0.9844 - val_loss: 1.7881 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 9.8560e-05\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 26/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980ms/step - accuracy: 0.7513 - auc: 0.9468 - loss: 1.8996 - precision: 1.0000 - recall: 0.7231\n",
            "Epoch 26: val_auc did not improve from 0.98828\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.7465 - auc: 0.9445 - loss: 1.9027 - precision: 1.0000 - recall: 0.7181 - val_accuracy: 0.9444 - val_auc: 0.9844 - val_loss: 1.7842 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 9.999999949475751e-05.\n",
            "Epoch 27/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6903 - auc: 0.8875 - loss: 1.9509 - precision: 0.9910 - recall: 0.6592   \n",
            "Epoch 27: val_auc did not improve from 0.98828\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.6929 - auc: 0.8881 - loss: 1.9487 - precision: 0.9910 - recall: 0.6611 - val_accuracy: 0.9444 - val_auc: 0.9844 - val_loss: 1.7262 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 9.999999898951501e-05.\n",
            "Epoch 28/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972ms/step - accuracy: 0.6638 - auc: 0.8940 - loss: 1.9404 - precision: 0.9991 - recall: 0.6335\n",
            "Epoch 28: val_auc did not improve from 0.98828\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.6639 - auc: 0.8899 - loss: 1.9442 - precision: 0.9984 - recall: 0.6337 - val_accuracy: 0.9444 - val_auc: 0.9727 - val_loss: 1.6881 - val_precision: 0.9688 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 9.99999984842725e-05.\n",
            "Epoch 29/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943ms/step - accuracy: 0.6619 - auc: 0.8061 - loss: 2.0128 - precision: 0.9486 - recall: 0.6531\n",
            "Epoch 29: val_auc did not improve from 0.98828\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.6626 - auc: 0.8106 - loss: 2.0090 - precision: 0.9505 - recall: 0.6530 - val_accuracy: 0.9722 - val_auc: 0.9766 - val_loss: 1.6913 - val_precision: 1.0000 - val_recall: 0.9688 - learning_rate: 1.0000e-04\n",
            "Epoch 29: early stopping\n",
            "Restoring model weights from the end of the best epoch: 14.\n",
            "\n",
            "=== EVALUATING MODEL ===\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 417ms/step - accuracy: 0.9630 - auc: 0.9713 - loss: 1.8869 - precision: 1.0000 - recall: 0.9593\n",
            "Test accuracy: 0.9730\n",
            "Test precision: 1.0000\n",
            "Test recall: 0.9697\n",
            "Test AUC: 0.9848\n",
            "\n",
            "Performing test-time augmentation for more robust predictions...\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 188ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 130ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step\n",
            "Optimal threshold found: 0.25 with F1 score: 0.9552\n",
            "\n",
            "ðŸ† === FINAL RESULTS WITH ADVANCED ENSEMBLE ===\n",
            "Accuracy: 0.9189\n",
            "Precision: 0.9412\n",
            "Recall: 0.9697\n",
            "F1: 0.9552\n",
            "Roc_auc: 0.9697\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Control       0.67      0.50      0.57         4\n",
            " Parkinson's       0.94      0.97      0.96        33\n",
            "\n",
            "    accuracy                           0.92        37\n",
            "   macro avg       0.80      0.73      0.76        37\n",
            "weighted avg       0.91      0.92      0.91        37\n",
            "\n",
            "\n",
            "Advanced ensemble model saved to /content/drive/MyDrive/parkinsons_lightweight_model_results/advanced_ensemble_model.keras\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Parkinson's Disease Prediction using Advanced Lightweight Ensemble\n",
        "# Combining the reliable training approach with MobileNetV2 + ShuffleNetV2 + GhostNet\n",
        "\n",
        "This notebook implements an optimized lightweight ensemble model to predict Parkinson's Disease\n",
        "from fMRI images, designed for small, imbalanced datasets with reliable training.\n",
        "\"\"\"\n",
        "\n",
        "# Mount Google Drive (for Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    import os\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "    else:\n",
        "        print(\"Google Drive already mounted.\")\n",
        "except ImportError:\n",
        "    pass  # Not running in Colab, skip mounting\n",
        "\n",
        "# All imports and code below (no pip install/uninstall, one-cell friendly)\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, roc_curve, auc,\n",
        "    roc_auc_score, precision_recall_curve, average_precision_score\n",
        ")\n",
        "import glob\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# --- GOOGLE DRIVE DATASET PATH SETUP ---\n",
        "PD_DIR = \"/content/drive/MyDrive/kaggle dataset/Disease_Prediction/PD_new\"\n",
        "CONTROL_DIR = \"/content/drive/MyDrive/kaggle dataset/Disease_Prediction/HC_new\"\n",
        "\n",
        "# --- VERIFY DATASET CONTENTS ---\n",
        "print(\"PD images:\", len(os.listdir(PD_DIR)) if os.path.exists(PD_DIR) else 0)\n",
        "print(\"Control images:\", len(os.listdir(CONTROL_DIR)) if os.path.exists(CONTROL_DIR) else 0)\n",
        "\n",
        "# Constants\n",
        "IMG_SIZE = 224  # Standard size for pre-trained models\n",
        "BATCH_SIZE = 16\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def get_image_files(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Warning: Directory {directory} does not exist\")\n",
        "        return []\n",
        "    jpg_files = glob.glob(os.path.join(directory, \"*.jpg\"))\n",
        "    png_files = glob.glob(os.path.join(directory, \"*.png\"))\n",
        "    return jpg_files + png_files\n",
        "\n",
        "pd_images = get_image_files(PD_DIR)\n",
        "control_images = get_image_files(CONTROL_DIR)\n",
        "\n",
        "print(f\"Parkinson's images: {len(pd_images)}\")\n",
        "print(f\"Control images: {len(control_images)}\")\n",
        "\n",
        "def process_path(file_path, label):\n",
        "    img = tf.io.read_file(file_path)\n",
        "    if tf.strings.regex_full_match(file_path, \".*\\.jpg\"):\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "    else:\n",
        "        img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
        "    img = tf.ensure_shape(img, [IMG_SIZE, IMG_SIZE, 3])\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    img = tf.image.per_image_standardization(img)\n",
        "    img = (img - tf.reduce_min(img)) / (tf.reduce_max(img) - tf.reduce_min(img))\n",
        "    return img, label\n",
        "\n",
        "# Test-time augmentation for more robust predictions\n",
        "def tta_predict(model, images, num_augmentations=10):\n",
        "    \"\"\"Test-time augmentation for more reliable predictions\"\"\"\n",
        "    # Create a basic augmentation pipeline for TTA\n",
        "    tta_aug = tf.keras.Sequential([\n",
        "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "        layers.RandomRotation(0.2),\n",
        "        layers.RandomZoom(0.1),\n",
        "        layers.RandomContrast(0.2)\n",
        "    ])\n",
        "\n",
        "    # Initialize predictions array\n",
        "    predictions = model.predict(images)  # Original prediction\n",
        "\n",
        "    # Add predictions with augmentations\n",
        "    for _ in range(num_augmentations-1):\n",
        "        aug_images = tta_aug(images, training=True)\n",
        "        predictions += model.predict(aug_images)\n",
        "\n",
        "    # Average predictions\n",
        "    return predictions / num_augmentations\n",
        "\n",
        "# Function to find the optimal threshold for classification\n",
        "def find_optimal_threshold(y_true, y_pred_prob):\n",
        "    \"\"\"Find the optimal threshold for binary classification\"\"\"\n",
        "    thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "    best_f1 = 0\n",
        "    best_threshold = 0.5\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_pred = (y_pred_prob > threshold).astype(int)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = threshold\n",
        "\n",
        "    print(f\"Optimal threshold found: {best_threshold:.2f} with F1 score: {best_f1:.4f}\")\n",
        "    return best_threshold\n",
        "\n",
        "# Helper function to create a dense block with regularization\n",
        "def _add_dense_block(x, units, dropout_rate=0.3):\n",
        "    \"\"\"Add a dense layer with batch normalization and dropout\"\"\"\n",
        "    x = layers.Dense(\n",
        "        units,\n",
        "        activation='relu',\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
        "    )(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return x\n",
        "\n",
        "# ShuffleNet block implementation\n",
        "def create_shufflenet_block(x, out_channels, stride=1):\n",
        "    \"\"\"Create a ShuffleNet block\"\"\"\n",
        "    input_channels = x.shape[-1]\n",
        "\n",
        "    # Create shortcut connection\n",
        "    if stride == 1 and input_channels == out_channels:\n",
        "        shortcut = x\n",
        "    else:\n",
        "        shortcut = layers.AveragePooling2D(pool_size=3, strides=stride, padding='same')(x)\n",
        "        shortcut = layers.Conv2D(out_channels, 1, padding='same')(shortcut)\n",
        "        shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    # Channel shuffle\n",
        "    x = layers.Conv2D(out_channels, 1, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # Depthwise convolution\n",
        "    x = layers.DepthwiseConv2D(3, strides=stride, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Pointwise convolution\n",
        "    x = layers.Conv2D(out_channels, 1, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Add shortcut\n",
        "    x = layers.Add()([shortcut, x])\n",
        "    x = layers.ReLU()(x)\n",
        "    return x\n",
        "\n",
        "# Ghost module implementation\n",
        "def create_ghost_module(x, out_channels, kernel_size=3, stride=1):\n",
        "    \"\"\"Create a Ghost module with consistent output shapes\"\"\"\n",
        "    # Ensure out_channels is even for proper splitting\n",
        "    primary_channels = out_channels // 2\n",
        "    ghost_channels = out_channels - primary_channels\n",
        "\n",
        "    # Primary convolution\n",
        "    primary = layers.Conv2D(primary_channels, kernel_size, strides=stride, padding='same')(x)\n",
        "    primary = layers.BatchNormalization()(primary)\n",
        "    primary = layers.ReLU()(primary)\n",
        "\n",
        "    # Ghost convolution - generate additional features from primary\n",
        "    ghost = layers.DepthwiseConv2D(kernel_size, strides=1, padding='same')(primary)\n",
        "    ghost = layers.BatchNormalization()(ghost)\n",
        "    ghost = layers.ReLU()(ghost)\n",
        "\n",
        "    # If we need more ghost channels, pad with zeros or duplicate\n",
        "    if ghost_channels > primary_channels:\n",
        "        # Add extra channels by duplicating some ghost features\n",
        "        extra_channels = ghost_channels - primary_channels\n",
        "        ghost_extra = layers.Conv2D(extra_channels, 1, padding='same')(ghost)\n",
        "        ghost = layers.Concatenate()([ghost, ghost_extra])\n",
        "    elif ghost_channels < primary_channels:\n",
        "        # Reduce ghost channels\n",
        "        ghost = layers.Conv2D(ghost_channels, 1, padding='same')(ghost)\n",
        "\n",
        "    # Concatenate primary and ghost features\n",
        "    return layers.Concatenate()([primary, ghost])\n",
        "\n",
        "# Create the advanced lightweight ensemble model\n",
        "def create_mobile_shuffle_ghost_ensemble():\n",
        "    \"\"\"Creates an ultra-high performance ensemble with attention mechanisms\"\"\"\n",
        "    inputs = layers.Input(shape=(224, 224, 3))  # Explicitly use 224x224\n",
        "\n",
        "    # MobileNetV2 branch with fine-tuning\n",
        "    mobilenet = tf.keras.applications.MobileNetV2(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(224, 224, 3),  # Explicitly use 224x224 for MobileNetV2\n",
        "        name='mobilenet_branch'\n",
        "    )\n",
        "    # Fine-tune last 30 layers for better performance\n",
        "    for layer in mobilenet.layers[:-30]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    mobilenet_output = mobilenet(inputs)\n",
        "    mobilenet_output = layers.GlobalAveragePooling2D()(mobilenet_output)\n",
        "    mobilenet_output = layers.BatchNormalization()(mobilenet_output)\n",
        "    mobilenet_output = layers.Dropout(0.3)(mobilenet_output)\n",
        "\n",
        "    # Enhanced ShuffleNetV2 branch\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "    # More ShuffleNet blocks for better feature extraction\n",
        "    x = create_shufflenet_block(x, 64, stride=2)   # 32 -> 64\n",
        "    x = create_shufflenet_block(x, 128, stride=2)  # 64 -> 128\n",
        "    x = create_shufflenet_block(x, 256, stride=2)  # 128 -> 256\n",
        "    x = create_shufflenet_block(x, 256, stride=1)  # 256 -> 256 (same channels)\n",
        "\n",
        "    shufflenet_output = layers.GlobalAveragePooling2D()(x)\n",
        "    shufflenet_output = layers.BatchNormalization()(shufflenet_output)\n",
        "    shufflenet_output = layers.Dropout(0.3)(shufflenet_output)\n",
        "\n",
        "    # Enhanced GhostNet branch\n",
        "    x = layers.Conv2D(24, 3, strides=2, padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # More Ghost modules for better feature extraction\n",
        "    x = create_ghost_module(x, 48, stride=2)   # 24 -> 48\n",
        "    x = create_ghost_module(x, 96, stride=2)   # 48 -> 96\n",
        "    x = create_ghost_module(x, 192, stride=2)  # 96 -> 192\n",
        "    x = create_ghost_module(x, 192, stride=1)  # 192 -> 192 (same channels)\n",
        "\n",
        "    ghostnet_output = layers.GlobalAveragePooling2D()(x)\n",
        "    ghostnet_output = layers.BatchNormalization()(ghostnet_output)\n",
        "    ghostnet_output = layers.Dropout(0.3)(ghostnet_output)\n",
        "\n",
        "    # Advanced feature fusion with attention mechanism\n",
        "    combined_features = layers.Concatenate()([mobilenet_output, shufflenet_output, ghostnet_output])\n",
        "\n",
        "    # Self-attention mechanism for feature weighting\n",
        "    attention_weights = layers.Dense(combined_features.shape[-1], activation='sigmoid', name='attention_weights')(combined_features)\n",
        "    weighted_features = layers.Multiply(name='weighted_features')([combined_features, attention_weights])\n",
        "\n",
        "    # Residual connection\n",
        "    final_features = layers.Add(name='residual_connection')([combined_features, weighted_features])\n",
        "\n",
        "    # Ultra-high performance classification head\n",
        "    x = layers.BatchNormalization()(final_features)\n",
        "    x = _add_dense_block(x, 512, dropout_rate=0.5)\n",
        "    x = _add_dense_block(x, 256, dropout_rate=0.4)\n",
        "    x = _add_dense_block(x, 128, dropout_rate=0.3)\n",
        "    x = _add_dense_block(x, 64, dropout_rate=0.2)\n",
        "\n",
        "    outputs = layers.Dense(1, activation='sigmoid', name='final_output')(x)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=outputs, name='ultra_performance_ensemble')\n",
        "\n",
        "# Cyclic learning rate function\n",
        "def cyclic_lr_schedule(epoch, initial_lr=5e-5, max_lr=1e-4, step_size=5):\n",
        "    \"\"\"Cyclic learning rate schedule\"\"\"\n",
        "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
        "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
        "    lr = initial_lr + (max_lr - initial_lr) * np.maximum(0, (1 - x))\n",
        "    return float(lr)\n",
        "\n",
        "# Helper function to save all visualizations\n",
        "def save_visualizations(test_labels, y_pred, y_pred_prob, history, model_dir):\n",
        "    \"\"\"Create and save all visualizations for model evaluation\"\"\"\n",
        "    # 1. Confusion Matrix\n",
        "    cm = confusion_matrix(test_labels, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Control', \"Parkinson's\"],\n",
        "                yticklabels=['Control', \"Parkinson's\"])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(os.path.join(model_dir, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Training History\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.title('Accuracy', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # AUC plot\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(history.history['auc'], label='Training')\n",
        "    plt.plot(history.history['val_auc'], label='Validation')\n",
        "    plt.title('Area Under Curve (AUC)', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('AUC', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Precision/Recall plot\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(history.history['precision'], label='Training Precision')\n",
        "    plt.plot(history.history['val_precision'], label='Validation Precision')\n",
        "    plt.plot(history.history['recall'], label='Training Recall')\n",
        "    plt.plot(history.history['val_recall'], label='Validation Recall')\n",
        "    plt.title('Precision and Recall', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Score', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(history.history['loss'], label='Training')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.title('Loss', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(model_dir, 'training_metrics.png'), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # 3. ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(test_labels, y_pred_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=14)\n",
        "    plt.ylabel('True Positive Rate', fontsize=14)\n",
        "    plt.title('Receiver Operating Characteristic', fontsize=16)\n",
        "    plt.legend(loc=\"lower right\", fontsize=14)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.savefig(os.path.join(model_dir, 'roc_curve.png'), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Precision-Recall Curve\n",
        "    precision_values, recall_values, _ = precision_recall_curve(test_labels, y_pred_prob)\n",
        "    avg_precision = average_precision_score(test_labels, y_pred_prob)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(recall_values, precision_values, lw=2, label=f'Precision-Recall curve (AP = {avg_precision:.3f})')\n",
        "    plt.xlabel('Recall', fontsize=14)\n",
        "    plt.ylabel('Precision', fontsize=14)\n",
        "    plt.title('Precision-Recall Curve', fontsize=16)\n",
        "    plt.legend(loc=\"best\", fontsize=14)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.savefig(os.path.join(model_dir, 'precision_recall_curve.png'), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # 5. Save metrics to CSV\n",
        "    pd.DataFrame(history.history).to_csv(os.path.join(model_dir, 'training_metrics.csv'), index=False)\n",
        "\n",
        "# Create a main function with proper flow\n",
        "def main():\n",
        "    # Create output directory\n",
        "    model_dir = '/content/drive/MyDrive/parkinsons_lightweight_model_results'\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    print(f\"ðŸ“ Saving all results to: {model_dir}\")\n",
        "\n",
        "    print(\"\\n=== PREPARING DATASET ===\")\n",
        "    # Get and balance image datasets\n",
        "    pd_images = get_image_files(PD_DIR)\n",
        "    control_images = get_image_files(CONTROL_DIR)\n",
        "\n",
        "    try:\n",
        "        print(f\"Original dataset - PD: {len(pd_images)}, Control: {len(control_images)}\")\n",
        "\n",
        "        # Create data splits\n",
        "        all_files = pd_images + control_images\n",
        "        labels = [1] * len(pd_images) + [0] * len(control_images)\n",
        "\n",
        "        train_files, temp_files, train_labels, temp_labels = train_test_split(\n",
        "            all_files, labels, test_size=0.3,\n",
        "            random_state=42, stratify=labels\n",
        "        )\n",
        "        val_files, test_files, val_labels, test_labels = train_test_split(\n",
        "            temp_files, temp_labels, test_size=0.5,\n",
        "            random_state=42, stratify=temp_labels\n",
        "        )\n",
        "\n",
        "        print(f\"Split dataset:\")\n",
        "        pd_train = sum(1 for l in train_labels if l == 1)\n",
        "        control_train = sum(1 for l in train_labels if l == 0)\n",
        "        pd_val = sum(1 for l in val_labels if l == 1)\n",
        "        control_val = sum(1 for l in val_labels if l == 0)\n",
        "        pd_test = sum(1 for l in test_labels if l == 1)\n",
        "        control_test = sum(1 for l in test_labels if l == 0)\n",
        "        print(f\"Train set: {pd_train} PD, {control_train} control\")\n",
        "        print(f\"Validation set: {pd_val} PD, {control_val} control\")\n",
        "        print(f\"Test set: {pd_test} PD, {control_test} control\")\n",
        "\n",
        "        # Create datasets\n",
        "        train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels))\n",
        "        val_ds = tf.data.Dataset.from_tensor_slices((val_files, val_labels))\n",
        "        test_ds = tf.data.Dataset.from_tensor_slices((test_files, test_labels))\n",
        "\n",
        "        # Map to images\n",
        "        train_ds = train_ds.map(lambda x, y: process_path(x, y), num_parallel_calls=AUTOTUNE)\n",
        "        val_ds = val_ds.map(lambda x, y: process_path(x, y), num_parallel_calls=AUTOTUNE)\n",
        "        test_ds = test_ds.map(lambda x, y: process_path(x, y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "        # Calculate class weights for the imbalanced dataset\n",
        "        total_samples = pd_train + control_train\n",
        "        weight_for_0 = (total_samples / (2.0 * control_train)) * 0.5  # Control class\n",
        "        weight_for_1 = (total_samples / (2.0 * pd_train)) * 1.5       # PD class\n",
        "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "        print(f\"Class weights - Control: {weight_for_0:.2f}, PD: {weight_for_1:.2f}\")\n",
        "\n",
        "        # Apply simple augmentation to avoid tensor shape issues\n",
        "        def safe_augment(image, label):\n",
        "            # Use simple augmentations that won't cause shape issues\n",
        "            image = tf.image.random_flip_left_right(image)\n",
        "            image = tf.image.random_flip_up_down(image)\n",
        "            image = tf.image.random_brightness(image, 0.2)\n",
        "            image = tf.image.random_contrast(image, 0.8, 1.2)\n",
        "            image = tf.image.random_saturation(image, 0.8, 1.2)\n",
        "            image = tf.image.random_hue(image, 0.1)\n",
        "            return image, label\n",
        "\n",
        "        # Apply augmentation then batch\n",
        "        train_ds = train_ds.map(safe_augment, num_parallel_calls=AUTOTUNE)\n",
        "        train_ds = train_ds.shuffle(buffer_size=1000, seed=42).repeat()\n",
        "        train_ds = train_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "        # For validation and test, just batch without augmentation\n",
        "        val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "        test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "        print(\"\\n=== CREATING ADVANCED ENSEMBLE MODEL ===\")\n",
        "        model = create_mobile_shuffle_ghost_ensemble()\n",
        "        print(f\"Model created with {model.count_params():,} parameters\")\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=[\n",
        "                'accuracy',\n",
        "                tf.keras.metrics.Precision(name='precision'),\n",
        "                tf.keras.metrics.Recall(name='recall'),\n",
        "                tf.keras.metrics.AUC(name='auc')\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Set up callbacks\n",
        "        callbacks = [\n",
        "            # Model checkpoints\n",
        "            ModelCheckpoint(\n",
        "                filepath=os.path.join(model_dir, 'best_model.h5'),\n",
        "                monitor='val_auc', mode='max',\n",
        "                save_best_only=True, verbose=1\n",
        "            ),\n",
        "            # Training optimization\n",
        "            EarlyStopping(\n",
        "                monitor='val_auc', patience=15,\n",
        "                restore_best_weights=True, verbose=1\n",
        "            ),\n",
        "            LearningRateScheduler(cyclic_lr_schedule, verbose=1),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss', factor=0.7, patience=7,\n",
        "                min_lr=1e-6, verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Calculate steps per epoch to use approximately all data\n",
        "        steps_per_epoch = len(train_files) // BATCH_SIZE + 1\n",
        "\n",
        "        # Train model\n",
        "        print(\"\\nTraining advanced ensemble model...\")\n",
        "        history = model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=30,\n",
        "            callbacks=callbacks,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            class_weight=class_weight\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        print(\"\\n=== EVALUATING MODEL ===\")\n",
        "        test_results = model.evaluate(test_ds, verbose=1)\n",
        "        print(f\"Test accuracy: {test_results[1]:.4f}\")\n",
        "        print(f\"Test precision: {test_results[2]:.4f}\")\n",
        "        print(f\"Test recall: {test_results[3]:.4f}\")\n",
        "        print(f\"Test AUC: {test_results[4]:.4f}\")\n",
        "\n",
        "        # Test-time augmentation evaluation\n",
        "        print(\"\\nPerforming test-time augmentation for more robust predictions...\")\n",
        "        test_images = np.vstack([images for images, _ in test_ds])\n",
        "        test_labels = np.hstack([labels.numpy() for _, labels in test_ds])\n",
        "\n",
        "        y_pred_prob = tta_predict(model, test_images)\n",
        "\n",
        "        # Find optimal threshold\n",
        "        threshold = find_optimal_threshold(test_labels, y_pred_prob)\n",
        "        y_pred = (y_pred_prob > threshold).astype(int)\n",
        "\n",
        "        # Ensure predictions match true labels length\n",
        "        y_pred = y_pred[:len(test_labels)]\n",
        "        y_pred_prob = y_pred_prob[:len(test_labels)]\n",
        "\n",
        "        # Calculate and display metrics\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(test_labels, y_pred),\n",
        "            'precision': precision_score(test_labels, y_pred, zero_division=0),\n",
        "            'recall': recall_score(test_labels, y_pred, zero_division=0),\n",
        "            'f1': f1_score(test_labels, y_pred, zero_division=0),\n",
        "            'roc_auc': roc_auc_score(test_labels, y_pred_prob) if len(np.unique(test_labels)) > 1 else 0.5\n",
        "        }\n",
        "\n",
        "        print(\"\\nðŸ† === FINAL RESULTS WITH ADVANCED ENSEMBLE ===\")\n",
        "        for metric_name, value in metrics.items():\n",
        "            print(f\"{metric_name.capitalize()}: {value:.4f}\")\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(test_labels, y_pred,\n",
        "                                   target_names=['Control', \"Parkinson's\"],\n",
        "                                   zero_division=0))\n",
        "\n",
        "        # Save visualizations and model\n",
        "        save_visualizations(test_labels, y_pred, y_pred_prob, history, model_dir)\n",
        "\n",
        "        # Save final model\n",
        "        model_save_path = os.path.join(model_dir, 'advanced_ensemble_model.keras')\n",
        "        model.save(model_save_path)\n",
        "        print(f\"\\nAdvanced ensemble model saved to {model_save_path}\")\n",
        "\n",
        "        return model, history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "# Call the main function when script is run\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}